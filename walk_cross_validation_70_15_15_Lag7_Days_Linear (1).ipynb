{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance\n",
        "!pip install pandas_market_calendars\n",
        "!pip install statsmodels\n",
        "!pip install xgboost\n",
        "!pip install matplotlib-venn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUuKe7if6Yjq",
        "outputId": "801d4f3b-a648-4fd3-d297-701b7f34eac3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.12/dist-packages (0.2.66)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.32.4)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.4.0)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.12/dist-packages (from yfinance) (3.18.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.13.5)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.15.0)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
            "Collecting pandas_market_calendars\n",
            "  Downloading pandas_market_calendars-5.1.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.12/dist-packages (from pandas_market_calendars) (2.2.2)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.12/dist-packages (from pandas_market_calendars) (2025.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from pandas_market_calendars) (2.9.0.post0)\n",
            "Collecting exchange-calendars>=3.3 (from pandas_market_calendars)\n",
            "  Downloading exchange_calendars-4.11.1-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.0.2)\n",
            "Collecting pyluach (from exchange-calendars>=3.3->pandas_market_calendars)\n",
            "  Downloading pyluach-2.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.12/dist-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.1)\n",
            "Collecting korean_lunar_calendar (from exchange-calendars>=3.3->pandas_market_calendars)\n",
            "  Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1->pandas_market_calendars) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->pandas_market_calendars) (1.17.0)\n",
            "Downloading pandas_market_calendars-5.1.1-py3-none-any.whl (127 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.4/127.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exchange_calendars-4.11.1-py3-none-any.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m208.9/208.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading korean_lunar_calendar-0.3.1-py3-none-any.whl (9.0 kB)\n",
            "Downloading pyluach-2.3.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: korean_lunar_calendar, pyluach, exchange-calendars, pandas_market_calendars\n",
            "Successfully installed exchange-calendars-4.11.1 korean_lunar_calendar-0.3.1 pandas_market_calendars-5.1.1 pyluach-2.3.0\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (0.14.5)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (2.0.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.16.2)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.2)\n",
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.12/dist-packages (1.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from matplotlib-venn) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from matplotlib-venn) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from matplotlib-venn) (1.16.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->matplotlib-venn) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->matplotlib-venn) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bKc4oAa6Q7O",
        "outputId": "6b943678-cf1e-427c-ed56-b246122294fe",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas_market_calendars in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.12/dist-packages (from pandas_market_calendars) (2.2.2)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.12/dist-packages (from pandas_market_calendars) (2025.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from pandas_market_calendars) (2.9.0.post0)\n",
            "Requirement already satisfied: exchange-calendars>=3.3 in /usr/local/lib/python3.12/dist-packages (from pandas_market_calendars) (4.11.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.0.2)\n",
            "Requirement already satisfied: pyluach in /usr/local/lib/python3.12/dist-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.3.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.12/dist-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.1)\n",
            "Requirement already satisfied: korean_lunar_calendar in /usr/local/lib/python3.12/dist-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1->pandas_market_calendars) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->pandas_market_calendars) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas_market_calendars\n",
        "!pip install yfinance scikit-learn xgboost matplotlib\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import pandas_market_calendars as mcal\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ------------------------------\n",
        "# 0. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î list ‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
        "# ------------------------------\n",
        "tickers = [\"AAPL\", \"AMD\", \"APP\", \"AVGO\", \"GOOG\", \"GOOGL\", \"META\", \"MSFT\", \"NVDA\", \"PLTR\"]\n",
        "\n",
        "# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà\n",
        "start_date = datetime(2022, 1, 1)\n",
        "end_date   = datetime(2024, 12, 31)\n",
        "\n",
        "all_data_list = []\n",
        "\n",
        "# ------------------------------\n",
        "# 1. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
        "# ------------------------------\n",
        "for ticker in tickers:\n",
        "    print(f\"üì• Downloading {ticker} ...\")\n",
        "    data = yf.download(ticker, start=start_date.strftime('%Y-%m-%d'), end=end_date.strftime('%Y-%m-%d'))\n",
        "\n",
        "    if data.empty:\n",
        "        print(f\"‚ùå No data for {ticker}\")\n",
        "        continue\n",
        "\n",
        "    data.reset_index(inplace=True)\n",
        "\n",
        "    if isinstance(data.columns, pd.MultiIndex):\n",
        "        data.columns = [' '.join(col).strip() if isinstance(col, tuple) else col for col in data.columns.values]\n",
        "\n",
        "    def clean_columns(cols):\n",
        "        cleaned = []\n",
        "        for col in cols:\n",
        "            if 'Date' in col:\n",
        "                cleaned.append('Date')\n",
        "            else:\n",
        "                cleaned.append(col.split()[0])\n",
        "        return cleaned\n",
        "\n",
        "    data.columns = clean_columns(data.columns)\n",
        "    data['Symbol'] = ticker.upper()\n",
        "\n",
        "    wanted_cols = ['Date', 'Close', 'High', 'Low', 'Open', 'Symbol']\n",
        "    data = data[[col for col in wanted_cols if col in data.columns]]\n",
        "\n",
        "    all_data_list.append(data)\n",
        "\n",
        "# Concatenate all dataframes in the list into a single dataframe\n",
        "flat_df = pd.concat(all_data_list, ignore_index=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flat_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-TTHAypzGg3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pandas_market_calendars as mcal\n",
        "\n",
        "# 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡πà‡∏ß‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á\n",
        "all_dates = pd.DataFrame({\n",
        "    'Date': pd.date_range(flat_df['Date'].min(), flat_df['Date'].max())\n",
        "})\n",
        "\n",
        "# 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ Symbol ‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô\n",
        "symbols = flat_df['Symbol'].unique()\n",
        "expanded_list = []\n",
        "\n",
        "for sym in symbols:\n",
        "    temp = all_dates.copy()\n",
        "    temp['Symbol'] = sym\n",
        "    expanded_list.append(temp)\n",
        "\n",
        "# ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Symbol ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô\n",
        "all_dates_symbols = pd.concat(expanded_list, ignore_index=True)\n",
        "\n",
        "# 3. ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏±‡∏ö flat_df (‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô)\n",
        "full_df = all_dates_symbols.merge(flat_df, on=['Date', 'Symbol'], how='left')\n",
        "\n",
        "# 4. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
        "full_df = full_df[['Date', 'Symbol', 'Open', 'High', 'Low', 'Close']]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k_8v-puIGsQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-42n5rIcGv7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# ‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡πÉ‡∏´‡πâ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô (Close, Open, High, Low) ‡πÅ‡∏¢‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞ Symbol\n",
        "# ----------------------------\n",
        "price_cols = ['Close', 'Open', 'High', 'Low']\n",
        "\n",
        "for col in price_cols:\n",
        "    full_df[col] = full_df.groupby('Symbol')[col].transform(\n",
        "        lambda x: x.interpolate(method='spline', order=3)  # spline cubic\n",
        "                      .interpolate(method='linear')        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏°‡∏µ NaN ‡πÄ‡∏ï‡∏¥‡∏°‡πÅ‡∏ö‡∏ö linear\n",
        "    )"
      ],
      "metadata": {
        "id": "EMuDKt1xzJ9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_df.groupby('Symbol').apply(lambda g: g.isnull().sum())"
      ],
      "metadata": {
        "id": "TMGMh2ukH0Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 2) ‡∏™‡∏£‡πâ‡∏≤‡∏á lag 7 ‡∏ß‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Close ‡πÅ‡∏¢‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞ Symbol\n",
        "# ----------------------------\n",
        "lag_cols = []\n",
        "for lag in range(1, 8):  # ‡∏™‡∏£‡πâ‡∏≤‡∏á lag ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà 1 ‡∏ñ‡∏∂‡∏á 7 ‡∏ß‡∏±‡∏ô\n",
        "    col_name = f'Close_lag{lag}'\n",
        "    # shift ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏° Symbol\n",
        "    full_df[col_name] = full_df.groupby('Symbol')['Close'].shift(lag)\n",
        "    lag_cols.append(col_name)\n",
        "\n",
        "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô NaN\n",
        "print(\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô NaN ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå:\")\n",
        "print(full_df[['Close'] + lag_cols].isna().sum())\n",
        "\n",
        "# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "cols_to_show = ['Date','Symbol','Close'] + lag_cols\n",
        "print(full_df[cols_to_show].head(15))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_MpZY2djH5Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_df = full_df.dropna(subset=['Close'] + lag_cols)"
      ],
      "metadata": {
        "id": "8i8Liw48H8SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "m4mGfRByH9Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Target: Close ‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ\n",
        "# ----------------------------\n",
        "full_df['Close_next'] = full_df.groupby('Symbol')['Close'].shift(-1)\n",
        "\n",
        "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÅ‡∏•‡πâ‡∏ß\n",
        "print(full_df[['Symbol','Close','Close_next']].head(10))\n",
        "\n",
        "full_df = full_df.dropna(subset=['Close_next'])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tyAwCt4HIDWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Feature Matrix ‡πÅ‡∏•‡∏∞ Target\n",
        "lag_cols = [f'Close_lag{i}' for i in range(1,8)]\n",
        "X_cols = lag_cols\n",
        "X = full_df[X_cols]\n",
        "y = full_df['Close_next']\n",
        "\n",
        "# 4) ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
        "print(\"Feature Matrix X:\")\n",
        "print(X.head(10))\n",
        "print(\"\\nTarget y:\")\n",
        "print(y.head(10))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WV40tmf8KZa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‡∏£‡∏ß‡∏° X ‡πÅ‡∏•‡∏∞ y ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß\n",
        "data_ml = pd.concat([full_df[['Symbol']], X, y], axis=1)\n",
        "\n",
        "# ‡∏•‡∏ö row ‡∏ó‡∏µ‡πà‡∏°‡∏µ NaN ‡πÉ‡∏ô X ‡∏´‡∏£‡∏∑‡∏≠ y\n",
        "data_ml_clean = data_ml.dropna().reset_index(drop=True)\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á dictionary ‡πÄ‡∏Å‡πá‡∏ö X, y ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Symbol\n",
        "symbol_data = {}\n",
        "for symbol, group in data_ml_clean.groupby('Symbol'):\n",
        "    X_sym = group[X_cols].reset_index(drop=True)\n",
        "    y_sym = group['Close_next'].reset_index(drop=True)\n",
        "    symbol_data[symbol] = {'X': X_sym, 'y': y_sym}\n",
        "\n",
        "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏∏‡πâ‡∏ô‡πÅ‡∏£‡∏Å\n",
        "first_symbol = list(symbol_data.keys())[0]\n",
        "print(f\"Symbol: {first_symbol}\")\n",
        "print(\"Feature Matrix X:\")\n",
        "print(symbol_data[first_symbol]['X'].head(10))\n",
        "print(\"\\nTarget y:\")\n",
        "print(symbol_data[first_symbol]['y'].head(10))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PXlghscnKe7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ================================\n",
        "# 1. SCALE DATA FOR ALL SYMBOLS\n",
        "# ================================\n",
        "\n",
        "# dictionary ‡πÄ‡∏Å‡πá‡∏ö scaled X ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Symbol\n",
        "symbol_scaled_data = {}\n",
        "\n",
        "for symbol, data in symbol_data.items():\n",
        "    X_sym = data['X']\n",
        "    y_sym = data['y']\n",
        "\n",
        "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á scaler ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏∏‡πâ‡∏ô‡∏ô‡∏µ‡πâ\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_sym)\n",
        "\n",
        "    # ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô DataFrame\n",
        "    X_scaled_df = pd.DataFrame(X_scaled, columns=X_cols)\n",
        "\n",
        "    # ‡πÄ‡∏Å‡πá‡∏ö‡∏•‡∏á dictionary\n",
        "    symbol_scaled_data[symbol] = {\n",
        "        'X': X_scaled_df,\n",
        "        'y': y_sym,\n",
        "        'scaler': scaler  # ‡πÄ‡∏Å‡πá‡∏ö scaler ‡πÑ‡∏ß‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï\n",
        "    }\n",
        "\n",
        "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏´‡∏∏‡πâ‡∏ô‡πÅ‡∏£‡∏Å\n",
        "first_symbol = list(symbol_scaled_data.keys())[0]\n",
        "print(f\"Symbol: {first_symbol}\")\n",
        "print(\"Feature Matrix X (scaled):\")\n",
        "print(symbol_scaled_data[first_symbol]['X'].head(10))\n",
        "print(\"\\nTarget y:\")\n",
        "print(symbol_scaled_data[first_symbol]['y'].head(10))\n",
        "\n",
        "# ================================\n",
        "# 2. SPLIT DATA FOR WALK-FORWARD (70-15-15)\n",
        "# ================================\n",
        "\n",
        "def split_data_70_15_15(X, y):\n",
        "    \"\"\"\n",
        "    ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô 70-15-15 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö time series\n",
        "    \"\"\"\n",
        "    train_ratio = 0.70\n",
        "    val_ratio = 0.15\n",
        "    test_ratio = 0.15\n",
        "\n",
        "    total_samples = len(X)\n",
        "    train_end_idx = int(total_samples * train_ratio)\n",
        "    val_end_idx = int(total_samples * (train_ratio + val_ratio))\n",
        "\n",
        "    # ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "    X_train = X[:train_end_idx]\n",
        "    X_val = X[train_end_idx:val_end_idx]\n",
        "    X_test = X[val_end_idx:]\n",
        "\n",
        "    y_train = y[:train_end_idx]\n",
        "    y_val = y[train_end_idx:val_end_idx]\n",
        "    y_test = y[val_end_idx:]\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏´‡∏∏‡πâ‡∏ô\n",
        "symbol_split_data = {}\n",
        "\n",
        "for symbol, data in symbol_scaled_data.items():\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = split_data_70_15_15(\n",
        "        data['X'], data['y']\n",
        "    )\n",
        "\n",
        "    symbol_split_data[symbol] = {\n",
        "        'X_train': X_train,\n",
        "        'X_val': X_val,\n",
        "        'X_test': X_test,\n",
        "        'y_train': y_train,\n",
        "        'y_val': y_val,\n",
        "        'y_test': y_test,\n",
        "        'scaler': data['scaler']\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìä {symbol} Data Split:\")\n",
        "    print(f\"  Train: {len(X_train)} samples (70%)\")\n",
        "    print(f\"  Val:   {len(X_val)} samples (15%)\")\n",
        "    print(f\"  Test:  {len(X_test)} samples (15%)\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "w4CblaRdKwuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 3. DEFINE MODELS\n",
        "# ================================\n",
        "\n",
        "models = {\n",
        "    'LinearRegression': LinearRegression(),\n",
        "    'Ridge': Ridge(alpha=1.0),\n",
        "    'Lasso': Lasso(alpha=0.1),\n",
        "    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'XGBoost': XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42, objective='reg:squarederror')\n",
        "}\n",
        "\n",
        "print(f\"\\nü§ñ Models to evaluate: {list(models.keys())}\")"
      ],
      "metadata": {
        "id": "X1EWoBKZMB8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 4. WALK-FORWARD VALIDATION FUNCTION\n",
        "# ================================\n",
        "\n",
        "def walk_forward_validation_single_stock(X_train, y_train, X_val, y_val, X_test, y_test, window_size, models):\n",
        "    \"\"\"\n",
        "    Walk-forward validation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏∏‡πâ‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏π‡∏Å scale ‡πÅ‡∏•‡πâ‡∏ß)\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    predictions = {}\n",
        "    val_predictions = {}\n",
        "\n",
        "    # Initialize results storage\n",
        "    for model_name in models.keys():\n",
        "        predictions[model_name] = []\n",
        "        val_predictions[model_name] = []\n",
        "        results.append({\n",
        "            'Model': model_name,\n",
        "            'Val_RMSE': [],\n",
        "            'Val_MAE': [],\n",
        "            'Val_R2': [],\n",
        "            'Test_RMSE': [],\n",
        "            'Test_MAE': [],\n",
        "            'Test_R2': []\n",
        "        })\n",
        "\n",
        "    # Combine train and validation for walk-forward\n",
        "    X_train_val = pd.concat([X_train, X_val])\n",
        "    y_train_val = pd.concat([y_train, y_val])\n",
        "\n",
        "    # Walk-forward validation on test set\n",
        "    for i in range(len(X_test)):\n",
        "        # Define training window (use recent data from train+val)\n",
        "        train_start = max(0, len(X_train_val) - window_size)\n",
        "        X_train_window = X_train_val.iloc[train_start:]\n",
        "        y_train_window = y_train_val.iloc[train_start:]\n",
        "\n",
        "        # Skip if training data is empty\n",
        "        if len(X_train_window) == 0:\n",
        "            continue\n",
        "\n",
        "        # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á scale ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏π‡∏Å scale ‡πÅ‡∏•‡πâ‡∏ß\n",
        "        X_train_array = X_train_window.values\n",
        "        X_test_array = X_test.iloc[[i]].values\n",
        "\n",
        "        # Predict on validation set (only once)\n",
        "        if i == 0:\n",
        "            X_val_array = X_val.values\n",
        "\n",
        "        # Train and predict with each model\n",
        "        for model_name, model in models.items():\n",
        "            try:\n",
        "                # Train model\n",
        "                model.fit(X_train_array, y_train_window)\n",
        "\n",
        "                # Predict on validation set (for model evaluation, only once)\n",
        "                if i == 0:\n",
        "                    y_val_pred = model.predict(X_val_array)\n",
        "                    val_predictions[model_name] = y_val_pred\n",
        "\n",
        "                # Predict on test point\n",
        "                y_test_pred = model.predict(X_test_array)[0]\n",
        "                predictions[model_name].append(y_test_pred)\n",
        "\n",
        "                # Calculate validation metrics (only once)\n",
        "                if i == 0:\n",
        "                    val_rmse = np.sqrt(mean_squared_error(y_val, val_predictions[model_name]))\n",
        "                    val_mae = mean_absolute_error(y_val, val_predictions[model_name])\n",
        "                    val_r2 = r2_score(y_val, val_predictions[model_name])\n",
        "\n",
        "                    model_idx = next(idx for idx, res in enumerate(results) if res['Model'] == model_name)\n",
        "                    results[model_idx]['Val_RMSE'] = val_rmse\n",
        "                    results[model_idx]['Val_MAE'] = val_mae\n",
        "                    results[model_idx]['Val_R2'] = val_r2\n",
        "\n",
        "                # Calculate test metrics (cumulative)\n",
        "                if len(predictions[model_name]) > 1:\n",
        "                    test_rmse = np.sqrt(mean_squared_error(y_test.iloc[:i+1], predictions[model_name]))\n",
        "                    test_mae = mean_absolute_error(y_test.iloc[:i+1], predictions[model_name])\n",
        "                    test_r2 = r2_score(y_test.iloc[:i+1], predictions[model_name])\n",
        "\n",
        "                    model_idx = next(idx for idx, res in enumerate(results) if res['Model'] == model_name)\n",
        "                    results[model_idx]['Test_RMSE'].append(test_rmse)\n",
        "                    results[model_idx]['Test_MAE'].append(test_mae)\n",
        "                    results[model_idx]['Test_R2'].append(test_r2)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Model {model_name} failed at iteration {i}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Add current test point to training data for next iteration\n",
        "        X_train_val = pd.concat([X_train_val, X_test.iloc[[i]]])\n",
        "        y_train_val = pd.concat([y_train_val, y_test.iloc[[i]]])\n",
        "\n",
        "    # Aggregate final results\n",
        "    final_results = []\n",
        "    for res in results:\n",
        "        # Validation metrics (single values)\n",
        "        val_rmse = res['Val_RMSE'] if isinstance(res['Val_RMSE'], (int, float)) else np.nan\n",
        "        val_mae = res['Val_MAE'] if isinstance(res['Val_MAE'], (int, float)) else np.nan\n",
        "        val_r2 = res['Val_R2'] if isinstance(res['Val_R2'], (int, float)) else np.nan\n",
        "\n",
        "        # Test metrics (mean of all iterations)\n",
        "        test_rmse_mean = np.mean(res['Test_RMSE']) if res['Test_RMSE'] else np.nan\n",
        "        test_rmse_std = np.std(res['Test_RMSE']) if res['Test_RMSE'] else np.nan\n",
        "        test_mae_mean = np.mean(res['Test_MAE']) if res['Test_MAE'] else np.nan\n",
        "        test_mae_std = np.std(res['Test_MAE']) if res['Test_MAE'] else np.nan\n",
        "        test_r2_mean = np.mean(res['Test_R2']) if res['Test_R2'] else np.nan\n",
        "        test_r2_std = np.std(res['Test_R2']) if res['Test_R2'] else np.nan\n",
        "\n",
        "        final_results.append({\n",
        "            'Model': res['Model'],\n",
        "            'Val_RMSE': val_rmse,\n",
        "            'Val_MAE': val_mae,\n",
        "            'Val_R2': val_r2,\n",
        "            'Test_RMSE': test_rmse_mean,\n",
        "            'Test_RMSE_std': test_rmse_std,\n",
        "            'Test_MAE': test_mae_mean,\n",
        "            'Test_MAE_std': test_mae_std,\n",
        "            'Test_R2': test_r2_mean,\n",
        "            'Test_R2_std': test_r2_std\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(final_results), predictions, val_predictions"
      ],
      "metadata": {
        "id": "MKzDCRmxMINU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 5. RUN WALK-FORWARD VALIDATION FOR ALL SYMBOLS\n",
        "# ================================\n",
        "\n",
        "# Set window size\n",
        "window_size = 60  # Approximately 1 year of trading days\n",
        "\n",
        "# ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡∏´‡∏∏‡πâ‡∏ô\n",
        "all_results = {}\n",
        "all_predictions = {}\n",
        "\n",
        "print(\"üöÄ Starting Walk-Forward Validation for all symbols...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for symbol in symbol_split_data.keys():\n",
        "    print(f\"\\nüìà Processing {symbol}...\")\n",
        "\n",
        "    data = symbol_split_data[symbol]\n",
        "\n",
        "    # Run walk-forward validation\n",
        "    wf_results, wf_test_predictions, wf_val_predictions = walk_forward_validation_single_stock(\n",
        "        data['X_train'], data['y_train'],\n",
        "        data['X_val'], data['y_val'],\n",
        "        data['X_test'], data['y_test'],\n",
        "        window_size, models\n",
        "    )\n",
        "\n",
        "    # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
        "    all_results[symbol] = wf_results\n",
        "    all_predictions[symbol] = {\n",
        "        'test_predictions': wf_test_predictions,\n",
        "        'val_predictions': wf_val_predictions,\n",
        "        'actual_test': data['y_test'],\n",
        "        'actual_val': data['y_val']\n",
        "    }\n",
        "\n",
        "    print(f\"‚úÖ {symbol} completed!\")\n",
        "    print(wf_results.round(4))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4WUO7qWQMOuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_predictions.to_csv('all_results.csv', index=False)"
      ],
      "metadata": {
        "id": "VpEopuG6kqsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 6. SUMMARY RESULTS ACROSS ALL SYMBOLS\n",
        "# ================================\n",
        "\n",
        "print(\"\\nüéØ SUMMARY: Best Models by Symbol\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary_results = []\n",
        "\n",
        "for symbol, results_df in all_results.items():\n",
        "    best_val_rmse = results_df.loc[results_df['Val_RMSE'].idxmin(), 'Model']\n",
        "    best_test_rmse = results_df.loc[results_df['Test_RMSE'].idxmin(), 'Model']\n",
        "    best_val_r2 = results_df.loc[results_df['Val_R2'].idxmax(), 'Model']\n",
        "    best_test_r2 = results_df.loc[results_df['Test_R2'].idxmax(), 'Model']\n",
        "\n",
        "    summary_results.append({\n",
        "        'Symbol': symbol,\n",
        "        'Best_Val_RMSE_Model': best_val_rmse,\n",
        "        'Val_RMSE': results_df['Val_RMSE'].min(),\n",
        "        'Best_Test_RMSE_Model': best_test_rmse,\n",
        "        'Test_RMSE': results_df['Test_RMSE'].min(),\n",
        "        'Best_Val_R2_Model': best_val_r2,\n",
        "        'Val_R2': results_df['Val_R2'].max(),\n",
        "        'Best_Test_R2_Model': best_test_r2,\n",
        "        'Test_R2': results_df['Test_R2'].max()\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_results)\n",
        "print(summary_df.round(4))\n",
        "\n",
        "# ================================\n",
        "# 7. SAVE RESULTS\n",
        "# ================================\n",
        "\n",
        "# Save individual results\n",
        "for symbol, results_df in all_results.items():\n",
        "    results_df.to_csv(f'walk_forward_results_{symbol}.csv', index=False)\n",
        "\n",
        "# Save summary\n",
        "summary_df.to_csv('walk_forward_summary_all_symbols.csv', index=False)\n",
        "\n",
        "# Save predictions\n",
        "for symbol, pred_data in all_predictions.items():\n",
        "    predictions_df = pd.DataFrame(pred_data['test_predictions'])\n",
        "    predictions_df['Actual'] = pred_data['actual_test'].values\n",
        "    predictions_df.to_csv(f'walk_forward_predictions_{symbol}.csv', index=False)\n",
        "\n",
        "print(f\"\\nüíæ Results saved for {len(all_results)} symbols\")\n",
        "print(\"‚úÖ Walk-Forward Validation completed for all symbols!\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MozRUGPUMVFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ================================\n",
        "# 6. PLOT ACTUAL vs PREDICTED\n",
        "# ================================\n",
        "for symbol, preds in all_predictions.items():\n",
        "    print(f\"\\nüìä Plotting Actual vs Predicted for {symbol}...\")\n",
        "\n",
        "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö (‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å RMSE)\n",
        "    results_df = all_results[symbol]\n",
        "    best_model_name = results_df.loc[results_df['Test_RMSE'].idxmin(), 'Model']\n",
        "\n",
        "    print(f\"üèÜ Best Model for {symbol}: {best_model_name}\")\n",
        "\n",
        "    actual = preds['actual_test'].values\n",
        "\n",
        "    # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
        "    y_pred = preds['test_predictions'][best_model_name]\n",
        "\n",
        "    # ‡∏Å‡∏£‡∏≠‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç\n",
        "    y_pred = np.array([p for p in y_pred if isinstance(p, (float, int))])\n",
        "    actual = actual[-len(y_pred):]  # ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(actual, label='Actual', color='black', linewidth=2)\n",
        "    plt.plot(y_pred, label='Predicted', color='red', linestyle='--', linewidth=1.8)\n",
        "\n",
        "    plt.title(f'Actual vs Predicted Prices - {symbol} ({best_model_name})', fontsize=14)\n",
        "    plt.xlabel('Time Steps', fontsize=12)\n",
        "    plt.ylabel('Close Price', fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "g02Fvq7dPHKo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}